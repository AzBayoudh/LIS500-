<!DOCTYPE html>
<html lang="en">
 <head>
   <meta charset="UTF-8" />
   <meta name="viewport" content="width=device-width, initial-scale=1.0" />
   <title>Machine Learning Project</title>

   <!-- Bootstrap -->
   <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
     integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">

   <!-- My styles -->
   <link href="styles.css" rel="stylesheet" />

   <!-- p5 + sound + ml5 for Teachable Machine -->
   <script src="https://cdn.jsdelivr.net/npm/p5@1.11.1/lib/p5.min.js"></script>
   <script src="https://cdn.jsdelivr.net/npm/p5@1.11.1/lib/addons/p5.sound.min.js"></script>
   <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>



 <body>
   <!-- Site header and navigation -->
   <header>
     <nav class="container my-3">
       <a class="btn btn-outline-primary btn-sm me-2" href="index.html">Home</a>
       <a class="btn btn-outline-primary btn-sm me-2" href="about.html">About Us</a>
       <a class="btn btn-outline-primary btn-sm me-2" href="resources.html">Resources</a>
       <a class="btn btn-outline-primary btn-sm me-2" href="hero.html">Tech Hero</a>
       <a class="btn btn-primary btn-sm me-2" href="machine-learning.html" aria-current="page">Machine Learning</a>
     </nav>
   </header>

   <main class="container">
     <h1>Teachable Machine: Voice Classifier</h1>
     <p>
       On this page, we‚Äôre showcasing a sound-based machine learning project we built together using Google‚Äôs
       Teachable Machine. Our goal was to teach a simple model to recognize <strong>YES</strong>, <strong>NO</strong>,
       and <strong>QUIET</strong> through voice input. It‚Äôs a small example of how AI systems ‚Äúlearn‚Äù from the data
       humans feed them ‚Äî and how bias can easily sneak into that process.
     </p>

     <div class="intro-box">
       <h2 class="h5">Live Demo</h2>
       <p>
         When your browser asks for microphone permission, click <strong>Allow</strong>. Then try saying
         ‚ÄúYES,‚Äù ‚ÄúNO,‚Äù or staying quiet and watch how the emoji changes in real time.
       </p>
       <p class="text-muted">
         The canvas below is generated with p5.js and ml5.js and shows our live Teachable Machine sound classifier.
       </p>
        <a href="https://editor.p5js.org/MomenJaber01/full/TuR_lFkJH" target="_blank"><strong>Click on this link to give it a try!</strong> </a>
     </div>

     <div class="intro-box">
       <h2 class="h5">Demo Video</h2>

       <p>
         This short video records our Teachable Machine model in action as we trigger each class.
       </p>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/LELSG6Frcv0?si=MhpRVL8-FF9wQE1T" allowfullscreen></iframe>

     </div>
     <div class="intro-box">
       <h2 class="h5">Project Statement &amp; Reflection</h2>

       <!-- üîπ Aziz‚Äôs Sections: Intro + Training -->
       <h3>Intro & Model Training</h3>
       <p>
         For this project, our group decided to build a Teachable Machine that could recognize sound. We wanted something
         simple but interactive, so we trained a model that listens for three categories ‚Äî <strong>YES</strong>,
         <strong>NO</strong>, and <strong>QUIET</strong>. The idea was to explore how training data shapes a model‚Äôs behavior
         and connect that experience to the ideas from Joy Buolamwini‚Äôs <em>Unmasking AI</em>.
       </p>

       <p>
         I handled most of the training process and the setup for connecting our model to the website. We started by recording
         voice samples,  each of us saying ‚ÄúYES‚Äù and ‚ÄúNO‚Äù multiple times and capturing a few seconds of silence for ‚ÄúQUIET.‚Äù
         Our first version didn‚Äôt perform great. The model kept mixing things up and reacting to background noise. That made
         us realize how much the results depend on the data you feed it. We retrained it with more balanced samples, focusing
         on clearer, consistent recordings.
       </p>

       <p>
         Even after that, the model recognized my voice better than my partners‚Äô. It struggled with different tones and accents,
         which really connected to what Joy Buolamwini writes about, how algorithmic bias starts from uneven data. If a system
         is only trained on a narrow range of input, it‚Äôll work best for people who match that input and less accurately for
         everyone else. It‚Äôs a small project, but it shows how bias can appear even without bad intentions.
       </p>

       <p>
         Once the model worked, I used <code>ml5.js</code> and <code>p5.js</code> to integrate it into our site. It took a few
         tries, I kept running into missing script errors and wrong model URLs, but eventually it came together. Watching the
         emoji respond in real time to our voices was honestly cool. It made all the debugging worth it and gave us something
         visual to share on this page.
       </p>

       <!-- üîπ Partner Placeholder: Testing & Demo -->
       <h3>[Partner 1: Testing & Demo]</h3>
       <p>
         [This section will cover how we tested the model together, recorded the demo video, and what we observed when different
         people or environments affected accuracy.]
       </p>

       <!-- üîπ Partner Placeholder: Reflection on Unmasking AI -->
       <p>
        Working on this project helped us to understand and better see the ideas that Joy Buolamwini raises in "Unmasking AI." Our small sound-recognition model showed  
        just how quickly algorithmic bias can appear when the training data is not diverse. Because we only recored a limited set of voices, the model performed better with our own  
        voices and seemed to struggle with other people. This experience reflected Buolamwini's point that AI systems often work best for the groups that are most represented
        in their training data. Even though our intentions were neutral, the model still ended up acting biased and unfair. This proves the point that bias doesn't have to require 
        bad intentions, it can come from limited and unbalanced data and information.
       </p>

       <p>
         Overall, our project taught us how much human choices shape the behaviour and fairness of these machine learning systems. We clearly saw that bias doesn't just come from big 
         companies or compelex systems, it can appear in something as simple as a small classroom project simply because of who records the data and what data is available.
         Working together showed us how teamwork can affect the final outcome, as each person's voice and ideas can really change how the model learns. The process actually reminded 
         us that building AI isn't something that is automatic or neutral, it requires very careful decisions and collaboration to make sure everyone is properly represented. 
         By the end of this project, we have gained a better understanding that responsible AI starts with thoughtful and inclusive human input, no matter how important or small the 
         project may be.
       </p>
     </div>

     <div class="intro-box">
       <h2 class="h5">Project Details</h2>
       <ul>
         <li><strong>Group Members:</strong> Aziz Bayoudh, Momen Jaber, [Partner 2 Name?]</li>
         <li><strong>Tool:</strong> Teachable Machine (Sound Project) + p5.js + ml5.js</li>
         <li><strong>GitHub Repo:</strong>
           <a href="https://github.com/AzBayoudh/LIS500-" target="_blank">View project code on GitHub</a>
         </li>
       </ul>
     </div>
   </main>
 </body>
</html>
